Title: Fully virtualized ZTP deployments with DCI
Date: 2024-11-22 10:00
Category: how-to
Tags: dci, dci-openshift-agent, dci-pipeline, gitops, ztp, libvirt, vms
Slug: ztp-vms
Author: Ramon Perez
Github: raperez
Summary: This blog post will show you how you can deploy a fully virtualized ZTP deployment, where both the hub and spoke cluster will be placed in virtual machines managed with KVM. For this purpose, DCI will be used as key continuous integration tool to support the installation of both the hub and spoke cluster. This is useful for quickly troubleshooting of issues, not depending on a full baremetal infrastructure for that. The differences between the deployment on baremetal and virtualized scenarios (even though DCI works seamlessly in both cases) will be presented, also highlighting the key aspects you need to have in mind when deploying this pipeline with virtual machines. Best practices and lessons learned will be also shared.

[TOC]

# Introduction

We have already described how we can use DCI to achieve Zero-Touch Provisioning (ZTP) deployments [in this blog post](gitops-ztp-with-dci.html). However, in that case, we were assuming that the spoke cluster is a (set of) baremetal server(s), not taking into account other, more lightweight alternatives, that can be more suitable for testing and troubleshooting.

That's the reason of writing this blog post, aiming at providing some advices and good practices to have in mind when deploying your ZTP pipeline with DCI but using virtual machines (VMs) instead of baremetal servers, mainly for the deployment of the spoke cluster, where we will see that there are some differences that you have to have in mind for having a smooth transition.

Why considering virtualized scenarios? This is mainly oriented for testing ZTP deployments in labs where we don't have enough compute resources to allocate physical machines to run the full hub-spoke deployment, so that relying in VMs would have the same result. Interesting use cases for this kind of deployments would be, for example, to test some specific workflows on a CI, daily basis, avoiding such a big resource consumption, so that, with just one server, you can deploy the whole infrastructure without any problem.

# Differences between a virtualized and a baremetal scenario

There are some considerations you may have in mind if you want to move from a baremetal to a virtualized environment:

- We need BMC access to the target server that will be configured as spoke cluster.
- You need to retrieve the proper `rootDeviceHints` to be used to identify the disk VM.
- Network connectivity between physical and virtual servers is required.
- Boot entries on the virtual machine may make an impact on the performance of the workflow.

Let's check each topic one by one.

## BMC access

Typically, you use the IP address of the baremetal server to access to the BMC, then using proper credentials to successfully login. This could be an example of the `bmcAddress` defined in a SiteConfig manifest, for the case of a Dell server, using the iDRAC and Redfish:

        bmcAddress: "idrac-virtualmedia://192.168.16.160/redfish/v1/Systems/System.Embedded.1"

However, this approach cannot be followed in the case of virtual machines, at least directly, since Redfish is not implemented on the virtual machines. For this to work, we need to use Redfish development tools, such as [sushy-tools](https://github.com/openstack/sushy-tools), in the server where the VM is deployed.

This would solve the problem regarding the IP address and port (the sushy-tools one) to use to refer to Redfish. But then, how do you identify the target server (VM) that we want to use? In this case, we need to retrieve the UUID of the VM and use it to compose the `bmcAddress` variable.

Having the following output from `virsh dumpxml` command:

        <domain type='kvm' id='729'>
            <name>sno</name>
            <uuid>96b951bc-4d37-4596-8d41-d6918a0c0423</uuid>
            ...

Then, we can build the `bmcAddress` in the following way:

        bmcAddress: "redfish-virtualmedia://192.168.16.19:8082/redfish/v1/Systems/96b951bc-4d37-4596-8d41-d6918a0c0423"

So, we use the IP address and port of the host server, together with the VM UUID at the end.

## Identifying the disk of the VM

For the `rootDeviceHints` field in the SiteConfig, you mainly need to include the path to the proper disk to be used during spoke installation. This needs to be extracted, again, from `virsh dumpxml` output. Just look for the disk device whose device attribute is `disk`:

        <devices>
            <disk type='file' device='disk'>
                <driver name='qemu' type='qcow2'/>
                <source file='/var/lib/libvirt/images/sno_main.qcow2' index='1'/>
                <backingStore/>
                <target dev='vda' bus='virtio'/>
                <alias name='virtio-disk0'/>
                <address type='pci' domain='0x0000' bus='0x04' slot='0x00' function='0x0'/>
            </disk>
        </devices>

In this case, we can seet the `rootDeviceHints` in the following way:

        rootDeviceHints:
            deviceName: "/dev/disk/by-path/pci-0000:04:00.0"

So, mainly, we include the disk address at the end of the file path, using `pci` type, `0000` domain, `04` bus, `00` slot and `0` function.

## Network connectivity

You need to ensure that the virtual machines are reachable from other components and servers of your internal network, so that these virtual machines can interact each other and with these other elements.

In our case, for example, we rely on a bridged network, where we assign a specific MAC address to each virtual machine we deploy in our environment, and then, we have some dnsmasq rules in the server jumphost that replies back to all DHCP queries, so that the virtual machines can achieve a specific IP address that is always reachable in the internal network.

For this, in the SiteConfig, we define something like this for the node:

        nodeNetwork:
            interfaces:
              - name: enp1s0
                macAddress: "52:54:00:00:0a:07"
            config:
              interfaces:
                - name: enp1s0
                  type: ethernet
                  state: up
                  ipv4:
                    enabled: true
                    dhcp: true

You can see that the MAC address is hardcoded, and that we enable DHCP in the network interface. Then, if having something like this in the server managing DHCP and DNS queries in your network:

        $ cat /etc/dnsmasq.d/<file>.conf
        dhcp-host=52:54:00:00:0a:07,192.168.16.107,sno.server.example.lab

You can have more information regarding this bridged configuration in our [previous blog post](distributed-libvirt-clusters.html) that reviews the deployment of distributed libvirt clusters. The approach here would be similar.

## Boot entries

Each time a virtual machine is configured as a spoke cluster, some boot entries are added and can affect to their performance in case you want to redeploy the spoke cluster, since the VM may have problems when trying to reboot and shut it down.

So, our recommendation is to cleanup these boot entries. For this, we have some roles that already performs this for you, e.g. [efi_boot_mgr role from ansible-collection-redhatci-cop](https://github.com/redhatci/ansible-collection-redhatci-ocp/tree/main/roles/efi_boot_mgr). This can be called after deploying your spoke cluster with DCI by using the flag variable `dci_delete_uefi_boot_entries`, setting it to `true`.

# Conclusion

We hope these instructions and advices are useful for your transition towards a virtualized ZTP scenario. The rest of interactions with DCI and related tools (such as dci-pipeline) are mainly the same, so the interface you use to deploy the cluster and have it ready for its usage is not changing.

In case of any doubts or questions, don't hesitate to reach Telco Partner CI team!
